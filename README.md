# Network Security Project

## ğŸ“‹ Project Overview

A comprehensive machine learning project for detecting phishing websites using network security data. This end-to-end ML pipeline includes data ingestion from MongoDB, data validation, transformation, model training with MLflow tracking, and deployment via FastAPI.

## ğŸ¯ Problem Statement

Phishing attacks are a major cybersecurity threat where attackers create fake websites to steal sensitive information. This project builds a machine learning system to classify URLs as legitimate or phishing based on various features extracted from network traffic and URL characteristics.

## ğŸ—ï¸ Project Architecture

The project follows a modular architecture with the following components:

```
Network Security Project
â”‚
â”œâ”€â”€ Data Ingestion        â†’ Fetches data from MongoDB
â”œâ”€â”€ Data Validation       â†’ Validates schema and detects data drift
â”œâ”€â”€ Data Transformation   â†’ Preprocesses and transforms features
â”œâ”€â”€ Model Training        â†’ Trains ML models with hyperparameter tuning
â””â”€â”€ Deployment            â†’ FastAPI web service for predictions
```

## ğŸš€ Features

- **Automated ML Pipeline**: End-to-end pipeline from data ingestion to model deployment
- **MongoDB Integration**: Seamless data fetching from cloud database
- **Data Drift Detection**: Monitors data quality and distribution changes
- **MLflow Tracking**: Experiment tracking and model versioning with DagsHub integration
- **Multiple ML Models**: Supports various classifiers (Random Forest, Gradient Boosting, AdaBoost, Decision Tree, Logistic Regression)
- **RESTful API**: FastAPI-based web service for real-time predictions
- **Modular Codebase**: Clean, maintainable code following software engineering best practices

## ğŸ“Š Dataset

**Dataset Name**: Phishing Data (`phisingData.csv`)

**Features**: 31 features including:

- URL characteristics (IP Address, URL Length, Shortening Service)
- Security indicators (SSL State, HTTPS Token, DNS Record)
- Content features (Links in tags, Pop-up Window, IFrame)
- Domain features (Age of Domain, Domain Registration Length)
- Web traffic metrics (Page Rank, Google Index, Web Traffic)

**Target Variable**: `Result` (1 = Legitimate, -1 = Phishing)

## ğŸ› ï¸ Tech Stack

### Core Technologies

- **Python 3.x**: Primary programming language
- **MongoDB**: Database for storing phishing data
- **FastAPI**: Web framework for API development
- **MLflow**: Experiment tracking and model registry
- **DagsHub**: MLOps platform for tracking experiments

### Machine Learning Libraries

- **scikit-learn**: ML algorithms and preprocessing
- **pandas**: Data manipulation
- **numpy**: Numerical computations
- **PyYAML**: Configuration management
- **dill**: Advanced object serialization

### Deployment & DevOps

- **Docker**: Containerization (configured)
- **Uvicorn**: ASGI server for FastAPI
- **python-dotenv**: Environment variable management

## ğŸ“ Project Structure

```
Network Security Project/
â”‚
â”œâ”€â”€ networksecurity/              # Main package
â”‚   â”œâ”€â”€ components/               # Pipeline components
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ data_validation.py
â”‚   â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”‚   â””â”€â”€ model_trainer.py
â”‚   â”œâ”€â”€ pipeline/                 # Training pipeline
â”‚   â”‚   â””â”€â”€ training_pipeline.py
â”‚   â”œâ”€â”€ entity/                   # Data classes
â”‚   â”‚   â”œâ”€â”€ config_entity.py
â”‚   â”‚   â””â”€â”€ artifact_entity.py
â”‚   â”œâ”€â”€ constant/                 # Constants and configs
â”‚   â”œâ”€â”€ exception/                # Custom exceptions
â”‚   â”œâ”€â”€ logging/                  # Logging utilities
â”‚   â”œâ”€â”€ utils/                    # Utility functions
â”‚   â”‚   â”œâ”€â”€ main_utils/
â”‚   â”‚   â””â”€â”€ ml_utils/
â”‚   â””â”€â”€ cloud/                    # Cloud storage utilities
â”‚
â”œâ”€â”€ Artifacts/                    # Training artifacts (timestamped)
â”œâ”€â”€ data_schema/                  # Data schema definitions
â”‚   â””â”€â”€ schema.yaml
â”œâ”€â”€ final_model/                  # Production model
â”œâ”€â”€ mlruns/                       # MLflow tracking
â”œâ”€â”€ templates/                    # HTML templates
â”œâ”€â”€ prediction_output/            # Prediction results
â”‚
â”œâ”€â”€ app.py                        # FastAPI application
â”œâ”€â”€ main.py                       # Pipeline execution script
â”œâ”€â”€ push_data.py                  # MongoDB data upload
â”œâ”€â”€ requirements.txt              # Dependencies
â”œâ”€â”€ setup.py                      # Package setup
â”œâ”€â”€ Dockerfile                    # Docker configuration
â””â”€â”€ README.md                     # This file
```

## ğŸ”§ Installation

### Prerequisites

- Python 3.8+
- MongoDB Atlas account (or local MongoDB)
- Git

### Setup Steps

1. **Clone the repository**

```bash
git clone <repository-url>
cd "Network Security Project"
```

2. **Create virtual environment**

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

4. **Configure environment variables**

Create a `.env` file in the project root:

```env
MONGODB_URL_KEY=your_mongodb_connection_string
MONGO_DB_URL=your_mongodb_connection_string
```

5. **Install package in development mode**

```bash
pip install -e .
```

## ğŸ’» Usage

### 1. Push Data to MongoDB (First Time Setup)

```bash
python push_data.py
```

### 2. Train the Model

```bash
python main.py
```

Or use the training pipeline:

```python
from networksecurity.pipeline.training_pipeline import TrainingPipeline

pipeline = TrainingPipeline()
pipeline.run_pipeline()
```

### 3. Run the FastAPI Application

```bash
uvicorn app:app --host 0.0.0.0 --port 8000
```

### 4. Access the API

- **Interactive API Docs**: http://localhost:8000/docs
- **Alternative Docs**: http://localhost:8000/redoc

### 5. Make Predictions

**Using the Web Interface**:

1. Navigate to http://localhost:8000/docs
2. Use the `/predict` endpoint
3. Upload a CSV file with the same features as training data
4. Get predictions in HTML table format

**Using cURL**:

```bash
curl -X POST "http://localhost:8000/predict" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@test.csv"
```

## ğŸ”¬ ML Pipeline Details

### 1. Data Ingestion

- Connects to MongoDB Atlas
- Exports data as Pandas DataFrame
- Splits data into train/test sets (80:20)
- Stores in feature store

### 2. Data Validation

- Validates schema against `schema.yaml`
- Checks column names and data types
- Detects data drift using statistical tests
- Generates drift reports

### 3. Data Transformation

- Handles missing values
- Feature engineering
- Applies preprocessing pipelines
- Saves preprocessor object

### 4. Model Training

- **Models Evaluated**:
  - Random Forest Classifier
  - Gradient Boosting Classifier
  - AdaBoost Classifier
  - Decision Tree Classifier
  - Logistic Regression

- **Evaluation Metrics**:
  - F1 Score
  - Precision
  - Recall

- **MLflow Integration**:
  - Logs parameters, metrics, and models
  - Tracks experiments in DagsHub
  - Model versioning and registry

### 5. Model Selection

- Selects best model based on F1 score
- Saves final model to `final_model/`
- Implements custom `NetworkModel` wrapper

## ğŸ“ˆ MLflow Tracking

The project uses MLflow with DagsHub for experiment tracking:

- **Repository**: https://dagshub.com/raunaqmittal/network-security
- **Tracked Metrics**: F1 Score, Precision, Recall
- **Tracked Parameters**: Model hyperparameters
- **Artifacts**: Trained models, preprocessors

## ğŸŒ API Endpoints

### GET `/`

- Redirects to API documentation

### GET `/train`

- Triggers the complete training pipeline
- Returns: Training status message

### POST `/predict`

- **Input**: CSV file with features
- **Output**: HTML table with predictions
- Saves predictions to `prediction_output/output.csv`

## ğŸ” Security & Best Practices

- Environment variables for sensitive data
- Custom exception handling
- Comprehensive logging system
- Modular and testable code structure
- Type hints for better code quality

## ğŸ“ Configuration

### Schema Configuration (`data_schema/schema.yaml`)

Defines:

- Column names and data types
- Numerical columns
- Feature validation rules

### Training Pipeline Constants

Located in `networksecurity/constant/training_pipeline/__init__.py`:

- Pipeline names and directories
- Train/test split ratios
- File naming conventions

## ğŸ› Error Handling

The project implements custom exception handling:

```python
from networksecurity.exception.exception import NetworkSecurityException
```

All exceptions are logged with detailed traceback information.

## ğŸ“Š Logging

Comprehensive logging system located in `networksecurity/logging/`:

- Logs stored in `networksecurity/logs/`
- Timestamped log files
- Multiple log levels (INFO, DEBUG, ERROR)

## ğŸš¢ Deployment

### Docker Deployment (To be configured)

```bash
docker build -t network-security .
docker run -p 8000:8000 network-security
```

### Cloud Deployment Options

- AWS EC2 / ECS
- Google Cloud Run
- Azure Container Instances
- Heroku

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is created for educational purposes.

## ğŸ‘¨â€ğŸ’» Author

**Raunaq Mittal**

- Email: raunaqmittal2004@gmail.com
- GitHub: [@raunaqmittal](https://github.com/raunaqmittal)

## ğŸ™ Acknowledgments

- Course: Udemy - Krish Naik's ML Project Series
- Dataset: Phishing Website Detection Dataset
- MLOps Platform: DagsHub

## ğŸ“ Support

For issues or questions:

1. Check existing issues on GitHub
2. Create a new issue with detailed description
3. Contact via email

---

**Project Status**: âœ… Active Development

**Last Updated**: January 2026
